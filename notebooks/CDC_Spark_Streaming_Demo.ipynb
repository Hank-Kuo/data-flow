{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDC Data Processing with Spark Streaming\n",
    "\n",
    "é€™å€‹ notebook å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Spark Streaming è™•ç†ä¾†è‡ª Kafka çš„ CDC (Change Data Capture) è³‡æ–™ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è¨­å®š Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# å»ºç«‹ Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDC-Analysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"âœ… Spark Session å·²å»ºç«‹\")\n",
    "print(f\"Spark UI: http://localhost:8088\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è®€å– Kafka CDC è³‡æ–™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–å®¢æˆ¶è³‡æ–™çš„è®Šæ›´\n",
    "customer_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"inventory-server.inventory.customers\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"âœ… Kafka stream å·²å»ºç«‹\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è§£æ CDC JSON è³‡æ–™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è§£æ JSON ä¸¦æå–è³‡æ–™\n",
    "parsed_df = customer_df.select(\n",
    "    col(\"timestamp\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.op\").alias(\"operation\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.after.id\").cast(\"int\").alias(\"customer_id\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.after.first_name\").alias(\"first_name\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.after.last_name\").alias(\"last_name\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.after.email\").alias(\"email\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.source.ts_ms\").cast(\"long\").alias(\"source_timestamp\")\n",
    ").filter(col(\"operation\").isNotNull())\n",
    "\n",
    "print(\"âœ… JSON è§£æé…ç½®å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å³æ™‚è³‡æ–™è™•ç† - é¡¯ç¤ºè®Šæ›´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©è™•ç†å‡½æ•¸\n",
    "def show_changes(batch_df, batch_id):\n",
    "    print(f\"\\n=== Batch {batch_id} ===\")\n",
    "    \n",
    "    if batch_df.count() > 0:\n",
    "        print(f\"æ”¶åˆ° {batch_df.count()} ç­†è®Šæ›´\")\n",
    "        \n",
    "        # æŒ‰æ“ä½œé¡å‹åˆ†çµ„\n",
    "        operations = batch_df.groupBy(\"operation\").count().collect()\n",
    "        for op in operations:\n",
    "            print(f\"  {op['operation']}: {op['count']} ç­†\")\n",
    "        \n",
    "        # é¡¯ç¤ºè³‡æ–™\n",
    "        batch_df.show(truncate=False)\n",
    "    else:\n",
    "        print(\"æ²’æœ‰æ–°çš„è®Šæ›´\")\n",
    "\n",
    "# å•Ÿå‹•ä¸²æµè™•ç†\n",
    "query = parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(show_changes) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"âœ… ä¸²æµè™•ç†å·²å•Ÿå‹•\")\n",
    "print(\"ğŸ’¡ ç¾åœ¨å¯ä»¥å˜—è©¦æ›´æ–°è³‡æ–™åº«ä¸­çš„å®¢æˆ¶è³‡æ–™ä¾†æŸ¥çœ‹å³æ™‚è®Šæ›´ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ¸¬è©¦è³‡æ–™æ›´æ–°\n",
    "\n",
    "åœ¨å¦ä¸€å€‹çµ‚ç«¯åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¾†è§¸ç™¼ CDC äº‹ä»¶ï¼š\n",
    "\n",
    "```bash\n",
    "# æ›´æ–°å®¢æˆ¶éƒµç®±\n",
    "docker exec postgres-cdc psql -U postgres -d inventory -c \"UPDATE inventory.customers SET email = 'updated@example.com' WHERE id = 1;\"\n",
    "\n",
    "# æ–°å¢å®¢æˆ¶\n",
    "docker exec postgres-cdc psql -U postgres -d inventory -c \"INSERT INTO inventory.customers (first_name, last_name, email) VALUES ('Test', 'User', 'test@example.com');\"\n",
    "\n",
    "# åˆªé™¤å®¢æˆ¶\n",
    "docker exec postgres-cdc psql -U postgres -d inventory -c \"DELETE FROM inventory.customers WHERE id = (SELECT MAX(id) FROM inventory.customers);\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. é€²éšåˆ†æ - å³æ™‚çµ±è¨ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹å³æ™‚çµ±è¨ˆ\n",
    "stats_df = parsed_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 minute\"),\n",
    "        col(\"operation\")\n",
    "    ).count() \\\n",
    "    .orderBy(\"window\")\n",
    "\n",
    "def show_stats(batch_df, batch_id):\n",
    "    if batch_df.count() > 0:\n",
    "        print(f\"\\nğŸ“Š å³æ™‚çµ±è¨ˆ (Batch {batch_id}):\")\n",
    "        batch_df.show(truncate=False)\n",
    "\n",
    "# åœæ­¢ä¹‹å‰çš„æŸ¥è©¢ï¼ˆå¦‚æœé‚„åœ¨é‹è¡Œï¼‰\n",
    "# query.stop()\n",
    "\n",
    "# å•Ÿå‹•çµ±è¨ˆæŸ¥è©¢\n",
    "stats_query = stats_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(show_stats) \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"âœ… å³æ™‚çµ±è¨ˆå·²å•Ÿå‹•\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. åœæ­¢ä¸²æµè™•ç†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœæ­¢æ‰€æœ‰æŸ¥è©¢\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"åœæ­¢ä¸²æµ: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ä¸²æµå·²åœæ­¢\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ‰¹æ¬¡åˆ†æç¯„ä¾‹\n",
    "\n",
    "é™¤äº†å³æ™‚ä¸²æµè™•ç†ï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥å° Kafka ä¸­çš„æ­·å²è³‡æ–™é€²è¡Œæ‰¹æ¬¡åˆ†æã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–æ‰€æœ‰æ­·å²è³‡æ–™é€²è¡Œæ‰¹æ¬¡åˆ†æ\n",
    "batch_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"inventory-server.inventory.customers\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# è§£æä¸¦åˆ†æ\n",
    "batch_parsed = batch_df.select(\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.op\").alias(\"operation\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.after.id\").cast(\"int\").alias(\"customer_id\"),\n",
    "    get_json_object(col(\"value\").cast(\"string\"), \"$.after.email\").alias(\"email\")\n",
    ").filter(col(\"operation\").isNotNull())\n",
    "\n",
    "print(\"ğŸ“Š æ­·å²è³‡æ–™çµ±è¨ˆ:\")\n",
    "batch_parsed.groupBy(\"operation\").count().show()\n",
    "\n",
    "print(f\"ç¸½è¨ˆè™•ç†äº† {batch_parsed.count()} ç­† CDC äº‹ä»¶\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
