# Data Flow

## ğŸ—ï¸ æ¶æ§‹æ¦‚è¦½

```
PostgreSQL CDC 
     â”‚
     â–¼
Debezium â†’ Kafka â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚       â”‚            â”‚
     â”‚       â–¼            â–¼
     â”‚  Schema Registry  Spark Streaming
     â”‚                       â”‚
     â”‚                       â–¼
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Airflow (ç›£æ§èˆ‡æ’ç¨‹)
                             â”‚
                             â–¼
                      Iceberg (MinIO)
                             â”‚
                             â–¼
                   Hive Metastore (metadata)
                             â”‚
                             â–¼
                          Trino
                             â”‚
                             â–¼
                         Superset
                             â”‚
                             â–¼
                             BI

Auth ï¼ˆApache Rangerï¼‰ï¼š
- Kafka topic
- Hive Metastore / Iceberg tables
- Trino: SQL search engine

Optional:
- Great Expectations/Soda â†’ Data Quality
- OpenMetadata â†’ Data Catalog / Lineage / Glossary
- dbt â†’ SQL Transformation model
```

## ğŸ¯ æŠ€è¡“å †ç–Š

- **Spark** - åˆ†æ•£å¼è¨ˆç®—å¼•æ“
- **Kafka + Debezium** - CDC è³‡æ–™ä¸²æµ
- **PostgreSQL** - æ¥­å‹™è³‡æ–™åº« (å•Ÿç”¨é‚è¼¯è¤‡è£½)
- **Airflow** - å·¥ä½œæµç¨‹ç®¡ç† (å¯é¸)
- **Jupyter** - äº’å‹•å¼åˆ†æç’°å¢ƒ

## ğŸ“Š è³‡æ–™æµç¨‹

```
PostgreSQL (æ¥­å‹™è³‡æ–™)
   â¬‡ (CDC / Debezium)
Kafka (Topics: inventory-server.*)
   â¬‡
Spark Structured Streaming (ETL, mapping, filter, join)
   â¬‡
å³æ™‚åˆ†æ & è­¦å ±
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å•Ÿå‹•åŸºç¤æœå‹™

```bash
# å•Ÿå‹• Kafka CDC ç³»çµ±
docker compose up -d zookeeper kafka schema-registry postgres-cdc kafka-connect kafka-ui debezium-ui

# è¨­å®š CDC é€£æ¥å™¨
./setup-cdc.sh
```

### 2. å•Ÿå‹• Spark åˆ†æ

```bash
# å•Ÿå‹• Spark å’Œ Jupyter
docker compose up -d spark-master spark-worker jupyter-spark

# åŸ·è¡Œ Spark streaming
./start-spark-streaming.sh
```

## ğŸ–¥ï¸ ç›£æ§ä»‹é¢
| æœå‹™ | URL | å¸³è™Ÿå¯†ç¢¼ |
|------|-----|---------|
| Kafka UI | http://localhost:8086 | - |
| Debezium UI | http://localhost:8087 | - |
| Spark Master | http://localhost:8088 | - |
| Jupyter Notebook | http://localhost:8888 | - |
| Airflow (å¯é¸) | http://localhost:8081 | admin/admin |

## ğŸ”¬ æ¸¬è©¦ CDC è³‡æ–™æµ

### è§¸ç™¼è³‡æ–™è®Šæ›´

```bash
# æ›´æ–°å®¢æˆ¶è³‡æ–™
docker exec postgres-cdc psql -U postgres -d inventory -c "UPDATE inventory.customers SET email = 'test@example.com' WHERE id = 1;"

# æ›´æ–°ç”¢å“åƒ¹æ ¼
docker exec postgres-cdc psql -U postgres -d inventory -c "UPDATE inventory.products SET price = 199.99 WHERE id = 1;"

# æ–°å¢è¨‚å–®
docker exec postgres-cdc psql -U postgres -d inventory -c "INSERT INTO inventory.orders (customer_id, total_amount, status) VALUES (1, 299.99, 'PENDING');"
```

### è§€å¯Ÿ Spark è™•ç†

Spark Streaming æœƒå³æ™‚é¡¯ç¤ºï¼š
- âœ… æ“ä½œçµ±è¨ˆ (æ’å…¥/æ›´æ–°/åˆªé™¤)
- ğŸ”” åƒ¹æ ¼è®Šæ›´è­¦å ±
- ğŸ“¦ åº«å­˜è®ŠåŒ–è¿½è¹¤
- ğŸ“Š å³æ™‚æŒ‡æ¨™çµ±è¨ˆ

## ğŸ’» Jupyter é–‹ç™¼

1. è¨ªå• http://localhost:8888
2. é–‹å•Ÿ `CDC_Spark_Streaming_Demo.ipynb`
3. åŸ·è¡Œäº’å‹•å¼ CDC è³‡æ–™åˆ†æ

## ğŸ“ è³‡æ–™è¡¨çµæ§‹

- **inventory.customers** - å®¢æˆ¶è³‡æ–™
- **inventory.products** - ç”¢å“è³‡æ–™
- **inventory.orders** - è¨‚å–®è³‡æ–™
- **inventory.order_items** - è¨‚å–®é …ç›®
- **inventory.addresses** - åœ°å€è³‡æ–™

## ğŸ› ï¸ è‡ªå®šç¾©é–‹ç™¼

### Spark Streaming æ‡‰ç”¨

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

spark = SparkSession.builder \
    .appName("Custom-CDC-App") \
    .master("spark://spark-master:7077") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0") \
    .getOrCreate()

# è™•ç† CDC è³‡æ–™
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:29092") \
    .option("subscribe", "inventory-server.inventory.customers") \
    .load()
```

### æª”æ¡ˆçµæ§‹

```
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ streaming/              # Spark æ‡‰ç”¨ç¨‹å¼
â”‚   â”œâ”€â”€ data/                   # è³‡æ–™ç›®éŒ„
â”‚   â””â”€â”€ logs/                   # æ—¥èªŒç›®éŒ„
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”œâ”€â”€ dags/                       # Airflow DAGs
â”œâ”€â”€ init-scripts/               # è³‡æ–™åº«åˆå§‹åŒ–
â””â”€â”€ setup-cdc.sh              # CDC è¨­å®šè…³æœ¬
```

## ğŸ¯ ä½¿ç”¨æ¡ˆä¾‹

1. **å³æ™‚ç›£æ§** - ç›£æ§è³‡æ–™åº«è®Šæ›´ï¼Œç”¢ç”Ÿå³æ™‚è­¦å ±
2. **ç•°å¸¸åµæ¸¬** - åˆ†æäº¤æ˜“æ¨¡å¼ï¼Œåµæ¸¬ç•°å¸¸è¡Œç‚º
3. **åº«å­˜ç®¡ç†** - è¿½è¹¤åº«å­˜è®ŠåŒ–ï¼Œè‡ªå‹•è§¸ç™¼è£œè²¨
4. **å®¢æˆ¶åˆ†æ** - å³æ™‚åˆ†æå®¢æˆ¶è¡Œç‚ºè®ŠåŒ–

## ğŸ”§ æ•…éšœæ’é™¤

```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
docker compose ps

# æŸ¥çœ‹æ—¥èªŒ
docker compose logs kafka-connect
docker compose logs spark-master

# é‡ç½®ç³»çµ±ï¼ˆæœƒæ¸…é™¤è³‡æ–™ï¼‰
docker compose down -v && docker compose up -d
```

---

ğŸ‰ **æ‚¨çš„å³æ™‚è³‡æ–™æµè™•ç†å¹³å°å·²å°±ç·’ï¼**

graph TD

%% Source
subgraph Source
    PG[PostgreSQL]
end

%% CDC Layer
subgraph CDC Layer
    Debezium[Debezium Connector]
    Kafka[Kafka]
    Registry[Confluent Schema Registry]
end

%% Processing Layer
subgraph Streaming & ETL
    Spark[Spark Streaming<br/>(Avro/JSON w/ Schema)]
    Checkpoint[Checkpoint / Offset Store]
end

%% Storage Layer
subgraph Lakehouse
    Iceberg[Apache Iceberg<br/>on MinIO (S3)]
    Hive[Hive Metastore]
end

%% Query/BI Layer
subgraph Query & BI
    Trino[Trino SQL Engine]
    Superset[Superset Dashboard]
end

%% Governance
subgraph Governance
    Ranger[Apache Ranger]
end

%% Modeling
subgraph Modeling
    dbt[dbt (on Trino)]
end

%% Arrows
PG --> Debezium --> Kafka
Kafka --> Registry
Kafka --> Spark
Registry --> Spark
Spark --> Iceberg
Spark --> Checkpoint
Hive --> Spark
Hive --> Trino
Iceberg --> Trino
Trino --> Superset
Trino --> dbt
Ranger --> Trino
Ranger --> Hive

Orchestration tool
- Dagster
- airflow
- argo

Others:
- OpenMetadata: Platform for data lineage(OpenLineage) and UI
- dbt: manage SQL and data lineage 
- Great Expectations: Data reconciliation
- Airbyte: CDC tool


```sql

WITH base AS (
  SELECT
    user_id,
    DATE(signup_time) AS signup_date,
    DATE(activity_time) AS activity_date
  FROM user_activity_table
),

cohorts AS (
  SELECT
    user_id,
    signup_date,
    activity_date,
    DATEDIFF(activity_date, signup_date) AS day_diff
  FROM base
  WHERE activity_date >= signup_date
),

daily_retention AS (
  SELECT
    signup_date,
    day_diff,
    COUNT(DISTINCT user_id) AS retained_users
  FROM cohorts
  GROUP BY signup_date, day_diff
),

cohort_sizes AS (
  SELECT
    signup_date,
    COUNT(DISTINCT user_id) AS cohort_size
  FROM base
  GROUP BY signup_date
)

SELECT
  d.signup_date,
  d.day_diff,
  d.retained_users,
  c.cohort_size,
  CAST(d.retained_users AS DOUBLE) / c.cohort_size AS retention_rate
FROM daily_retention d
JOIN cohort_sizes c ON d.signup_date = c.signup_date
ORDER BY signup_date, day_diff
;
```
